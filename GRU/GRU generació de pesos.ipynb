{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819bab89",
   "metadata": {},
   "source": [
    "MODEL GRU - GENERACIÓ DE PREUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 13s]\n",
      "val_loss: 0.06181293725967407\n",
      "\n",
      "Best val_loss So Far: 0.018125729635357857\n",
      "Total elapsed time: 00h 46m 47s\n",
      "Results summary\n",
      "Results in hyperparam_tuning\\S&P500_Stock_Price_output\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 3\n",
      "gru_units: 32\n",
      "dropout_rate: 0.1\n",
      "l2_reg: 0.001\n",
      "dense_units: 64\n",
      "learning_rate: 0.007487658008462713\n",
      "Score: 0.018125729635357857\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 16\n",
      "kernel_size: 3\n",
      "gru_units: 32\n",
      "dropout_rate: 0.2\n",
      "l2_reg: 0.001\n",
      "dense_units: 64\n",
      "learning_rate: 0.007783541188537225\n",
      "Score: 0.023036755621433258\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 16\n",
      "kernel_size: 3\n",
      "gru_units: 128\n",
      "dropout_rate: 0.1\n",
      "l2_reg: 0.0001\n",
      "dense_units: 16\n",
      "learning_rate: 0.0069405535544335905\n",
      "Score: 0.02695753052830696\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 5\n",
      "gru_units: 64\n",
      "dropout_rate: 0.30000000000000004\n",
      "l2_reg: 0.001\n",
      "dense_units: 64\n",
      "learning_rate: 0.005251469314834361\n",
      "Score: 0.027180766686797142\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 3\n",
      "gru_units: 32\n",
      "dropout_rate: 0.2\n",
      "l2_reg: 0.0001\n",
      "dense_units: 32\n",
      "learning_rate: 0.0031380270528399827\n",
      "Score: 0.029620284214615822\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 5\n",
      "gru_units: 128\n",
      "dropout_rate: 0.2\n",
      "l2_reg: 0.001\n",
      "dense_units: 32\n",
      "learning_rate: 0.0044558929106897766\n",
      "Score: 0.03178829699754715\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 5\n",
      "gru_units: 32\n",
      "dropout_rate: 0.2\n",
      "l2_reg: 0.0001\n",
      "dense_units: 32\n",
      "learning_rate: 0.0005511246797862368\n",
      "Score: 0.038047946989536285\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 32\n",
      "kernel_size: 3\n",
      "gru_units: 32\n",
      "dropout_rate: 0.30000000000000004\n",
      "l2_reg: 0.0001\n",
      "dense_units: 64\n",
      "learning_rate: 0.002793838236735594\n",
      "Score: 0.04478715732693672\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 16\n",
      "kernel_size: 5\n",
      "gru_units: 128\n",
      "dropout_rate: 0.2\n",
      "l2_reg: 0.0001\n",
      "dense_units: 32\n",
      "learning_rate: 0.0006190058359322792\n",
      "Score: 0.04852069541811943\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "conv_filters: 16\n",
      "kernel_size: 3\n",
      "gru_units: 32\n",
      "dropout_rate: 0.1\n",
      "l2_reg: 0.001\n",
      "dense_units: 32\n",
      "learning_rate: 0.0012634173195789128\n",
      "Score: 0.048983484506607056\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 28 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0689 - mae: 0.1880 - val_loss: 0.0774 - val_mae: 0.2102\n",
      "Epoch 2/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0292 - mae: 0.1147 - val_loss: 0.0240 - val_mae: 0.1168\n",
      "Epoch 3/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0325 - mae: 0.1264 - val_loss: 0.0593 - val_mae: 0.1792\n",
      "Epoch 4/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0307 - mae: 0.1167 - val_loss: 0.0368 - val_mae: 0.1364\n",
      "Epoch 5/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0286 - mae: 0.1150 - val_loss: 0.0792 - val_mae: 0.2109\n",
      "Epoch 6/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0328 - mae: 0.1175 - val_loss: 0.0523 - val_mae: 0.1663\n",
      "Epoch 7/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0352 - mae: 0.1198 - val_loss: 0.0278 - val_mae: 0.1252\n",
      "Epoch 8/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0257 - mae: 0.1083 - val_loss: 0.0292 - val_mae: 0.1281\n",
      "Epoch 9/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0254 - mae: 0.1116 - val_loss: 0.0350 - val_mae: 0.1359\n",
      "Epoch 10/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0274 - mae: 0.1082 - val_loss: 0.0922 - val_mae: 0.2335\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "VALIDATION → S&P500_Stock_Price_output.csv MSE: 5045.7697, MAE: 60.8977, R²: 0.9529\n",
      "TEST       → S&P500_Stock_Price_output.csv MSE: 182943.7435, MAE: 392.9776, R²: -1.9711\n",
      "\n",
      "Carpeta de resultats creada a: resultats_GRU_Attention\\s&p500_stock_price_output\n",
      "  ✓ Hiperparàmetres guardats a: resultats_GRU_Attention\\s&p500_stock_price_output\\best_hyperparameters.json\n",
      "  ✓ Pesos guardats a: resultats_GRU_Attention\\s&p500_stock_price_output\\best_model_weights.weights.h5\n",
      "  ✓ Mètriques guardades a: resultats_GRU_Attention\\s&p500_stock_price_output\\metrics_summary.csv\n",
      "  ✗ Carpeta de tuning esborrada: hyperparam_tuning\\S&P500_Stock_Price_output\n",
      "\n",
      "\n",
      "===== Procés complet finalitzat per a tots els datasets =====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout, Input, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import itertools\n",
    "import random\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Funció per crear seqüències temporals\n",
    "def crear_sequencies(X, y, look_back):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - look_back):\n",
    "        X_seq.append(X[i:(i + look_back), :])\n",
    "        y_seq.append(y[i + look_back])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Capa d’atenció simple\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"attn_W\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        super().build(input_shape)\n",
    "    def call(self, inputs):\n",
    "        score   = tf.matmul(tf.tanh(inputs), self.W)   # (batch, timesteps, 1)\n",
    "        weights = tf.nn.softmax(score, axis=1)          # (batch, timesteps, 1)\n",
    "        context = tf.reduce_sum(weights * inputs, axis=1)  # (batch, features)\n",
    "        return context\n",
    "\n",
    "# Funció build_model per a KerasTuner\n",
    "def build_model(hp, look_back, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back, n_features)))\n",
    "\n",
    "    # Conv1D + MaxPooling1D\n",
    "    conv_filters = hp.Choice('conv_filters', [16, 32, 64])\n",
    "    kernel_size  = hp.Choice('kernel_size', [3, 5])\n",
    "    model.add(Conv1D(filters=conv_filters,\n",
    "                     kernel_size=kernel_size,\n",
    "                     activation='relu',\n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Capa Bidirectional GRU\n",
    "    gru_units    = hp.Choice('gru_units', [32, 64, 128])\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.1, 0.4, step=0.1)\n",
    "    reg_factor   = hp.Choice('l2_reg', [1e-4, 1e-3])\n",
    "\n",
    "    model.add(Bidirectional(\n",
    "        GRU(units=gru_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            kernel_regularizer=regularizers.l2(reg_factor)\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    # Capa d’atenció\n",
    "    model.add(Attention())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Capa densa intermèdia\n",
    "    dense_units = hp.Choice('dense_units', [16, 32, 64])\n",
    "    model.add(Dense(dense_units,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(reg_factor)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Capa de sortida\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Optimitzador amb learning rate variable\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "    optimizer     = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Llista de fitxers a processar\n",
    "dataset_files = [\n",
    "    \"Amazon_Stock_Price_output.csv\",\n",
    "    \"Euro_Stoxx_50_Stock_Price_output.csv\",\n",
    "    \"Google_Stock_Price_output.csv\",\n",
    "    \"Hang_Seng_Stock_Price_output.csv\",\n",
    "    \"IBEX_35_Stock_Price_output.csv\",\n",
    "    \"Indra_Stock_Price_output.csv\",\n",
    "    \"P&G_Stock_Price_output.csv\",\n",
    "    \"S&P500_Stock_Price_output.csv\"\n",
    "]\n",
    "\n",
    "BASE_PATH = r\"C:\\Users\\jesus\\Desktop\\TFG\\GitHUb\\TFG_PredictStock\\Conjunt de dades Preprocessades\\Datasets\"\n",
    "look_back = 20  # finestra de 20 dies\n",
    "\n",
    "for file_name in dataset_files:\n",
    "    print(f\"\\n\\n====== Processant {file_name} ======\\n\")\n",
    "\n",
    "    # Carrega i preprocessament de dades \n",
    "    file_path = os.path.join(BASE_PATH, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=False)\n",
    "    df.sort_values('Date', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Llista de features i target\n",
    "    features = [\n",
    "        'Open', 'High', 'Low', 'Volume',\n",
    "        'EMA_7', 'EMA_40', 'MACD', 'Signal_Line',\n",
    "        'MACD_Hist', 'RSI', 'ATR'\n",
    "    ]\n",
    "    target = 'Close'\n",
    "\n",
    "    # Dropna de files amb NaNs a features o target\n",
    "    df.dropna(subset=features + [target], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Split cronològic train/val/test\n",
    "    n_total    = len(df)\n",
    "    train_size = int(n_total * 0.70)\n",
    "    val_size   = int(n_total * 0.15)\n",
    "\n",
    "    X_raw = df[features].values\n",
    "    y_raw = df[target].values\n",
    "\n",
    "    X_train_raw = X_raw[:train_size]\n",
    "    y_train_raw = y_raw[:train_size]\n",
    "\n",
    "    X_val_raw   = X_raw[train_size : train_size + val_size]\n",
    "    y_val_raw   = y_raw[train_size : train_size + val_size]\n",
    "\n",
    "    X_test_raw  = X_raw[train_size + val_size :]\n",
    "    y_test_raw  = y_raw[train_size + val_size :]\n",
    "\n",
    "    # Escalat de X i y \n",
    "    scaler_X = StandardScaler()\n",
    "    X_train  = scaler_X.fit_transform(X_train_raw)\n",
    "    X_val    = scaler_X.transform(X_val_raw)\n",
    "    X_test   = scaler_X.transform(X_test_raw)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train  = scaler_y.fit_transform(y_train_raw.reshape(-1,1)).ravel()\n",
    "    y_val    = scaler_y.transform(y_val_raw.reshape(-1,1)).ravel()\n",
    "    y_test   = scaler_y.transform(y_test_raw.reshape(-1,1)).ravel()\n",
    "\n",
    "    # Creació de seqüències temporals \n",
    "    X_train_seq, y_train_seq = crear_sequencies(X_train, y_train, look_back)\n",
    "    X_val_seq,   y_val_seq   = crear_sequencies(X_val,   y_val,   look_back)\n",
    "    X_test_seq,  y_test_seq  = crear_sequencies(X_test,  y_test,  look_back)\n",
    "\n",
    "    n_features = X_train_seq.shape[2]\n",
    "\n",
    "    # Cerca d’hiperparàmetres amb KerasTuner\n",
    "    tuner_dir     = \"hyperparam_tuning\"\n",
    "    tuner_project = file_name.replace(\".csv\", \"\")\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model(hp, look_back, n_features),\n",
    "        objective='val_loss',\n",
    "        max_trials=20,\n",
    "        executions_per_trial=1,\n",
    "        directory=tuner_dir,\n",
    "        project_name=tuner_project\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stop_tuner = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stop_tuner],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    # Obtenir el millor model i hiperparàmetres\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_hp_values = best_hp.values\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Fine‐tuning del millor model\n",
    "    fine_tune_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=8,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    best_model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[fine_tune_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Avaluació sobre validation i test i càlcul de mètriques\n",
    "    # Validation\n",
    "    y_val_pred_scaled = best_model.predict(X_val_seq)\n",
    "    y_val_pred        = scaler_y.inverse_transform(y_val_pred_scaled).ravel()\n",
    "    y_val_true        = scaler_y.inverse_transform(y_val_seq.reshape(-1,1)).ravel()\n",
    "\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "    mae_val = mean_absolute_error(y_val_true, y_val_pred)\n",
    "    r2_val  = r2_score(y_val_true, y_val_pred)\n",
    "\n",
    "    # Test\n",
    "    y_test_pred_scaled = best_model.predict(X_test_seq)\n",
    "    y_test_pred        = scaler_y.inverse_transform(y_test_pred_scaled).ravel()\n",
    "    y_test_true        = scaler_y.inverse_transform(y_test_seq.reshape(-1,1)).ravel()\n",
    "\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "    mae_test = mean_absolute_error(y_test_true, y_test_pred)\n",
    "    r2_test  = r2_score(y_test_true, y_test_pred)\n",
    "\n",
    "    print(f\"\\nVALIDATION → {file_name} RMSE: {rmse_val:.4f}, MAE: {mae_val:.4f}, R²: {r2_val:.4f}\")\n",
    "    print(f\"TEST       → {file_name} RMSE: {rmse_test:.4f}, MAE: {mae_test:.4f}, R²: {r2_test:.4f}\")\n",
    "\n",
    "    # Creació carpeta de resultats per a aquest dataset\n",
    "    dataset_name = file_name.replace(\".csv\", \"\")\n",
    "    results_base = \"resultats_GRU_Attention\"\n",
    "    model_folder = os.path.join(results_base, dataset_name.lower())\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    print(f\"\\nCarpeta de resultats creada a: {model_folder}\")\n",
    "\n",
    "    # Guardar els millors hiperparàmetres en JSON \n",
    "    hp_json_path = os.path.join(model_folder, \"best_hyperparameters.json\")\n",
    "    with open(hp_json_path, \"w\") as f:\n",
    "        json.dump(best_hp_values, f, indent=2)\n",
    "    print(f\"  ✓ Hiperparàmetres guardats a: {hp_json_path}\")\n",
    "\n",
    "    # Guardar només els pesos finals en HDF5 (.h5) \n",
    "    weights_path = os.path.join(model_folder, \"best_model_weights.weights.h5\")\n",
    "    best_model.save_weights(weights_path)\n",
    "    print(f\"  ✓ Pesos guardats a: {weights_path}\")\n",
    "\n",
    "    # Guardar les mètriques finals (validation + test) en un CSV\n",
    "    metrics = pd.DataFrame({\n",
    "        \"Dataset\":       [dataset_name],\n",
    "        \"Val_MAE\":       [mae_val],\n",
    "        \"Val_RMSE\":      [rmse_val],\n",
    "        \"Val_R2\":        [r2_val],\n",
    "        \"Test_MAE\":      [mae_test],\n",
    "        \"Test_RMSE\":     [rmse_test],\n",
    "        \"Test_R2\":       [r2_test]\n",
    "    })\n",
    "    metrics_csv = os.path.join(model_folder, \"metrics_summary.csv\")\n",
    "    metrics.to_csv(metrics_csv, index=False)\n",
    "    print(f\"  ✓ Mètriques guardades a: {metrics_csv}\")\n",
    "\n",
    "    # Esborrar la carpeta de KerasTuner (tots els trials)\n",
    "    tuner_path = os.path.join(tuner_dir, tuner_project)\n",
    "    if os.path.isdir(tuner_path):\n",
    "        shutil.rmtree(tuner_path)\n",
    "        print(f\"  ✗ Carpeta de tuning esborrada: {tuner_path}\")\n",
    "\n",
    "print(\"\\n\\n===== Procés complet finalitzat per a tots els datasets =====\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
