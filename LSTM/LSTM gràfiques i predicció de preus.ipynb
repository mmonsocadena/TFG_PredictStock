{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Procesando: Amazon_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\Amazon_Stock_Price_output\\Amazon_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para Amazon_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: Euro_Stoxx_50_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\Euro_Stoxx_50_Stock_Price_output\\Euro_Stoxx_50_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para Euro_Stoxx_50_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: Google_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\Google_Stock_Price_output\\Google_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para Google_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: Hang_Seng_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\Hang_Seng_Stock_Price_output\\Hang_Seng_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para Hang_Seng_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: IBEX_35_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\IBEX_35_Stock_Price_output\\IBEX_35_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para IBEX_35_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: Indra_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\Indra_Stock_Price_output\\Indra_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para Indra_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: P&G_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\P&G_Stock_Price_output\\P&G_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para P&G_Stock_Price_output, omitiendo.\n",
      "\n",
      "===== Procesando: S&P500_Stock_Price_output =====\n",
      "False\n",
      "results_LSTM\\S&P500_Stock_Price_output\\S&P500_Stock_Price_output_best_params.json\n",
      "  ⚠️ Faltan pesos o parámetros para S&P500_Stock_Price_output, omitiendo.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# CONFIGURACIÓN\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "BASE_PATH    = \"Conjunt de dades Preprocessades/Datasets\"\n",
    "RESULTS_PATH = \"results_LSTM\"\n",
    "DATASETS     = [\n",
    "    \"Amazon_Stock_Price_output.csv\",\n",
    "    \"Euro_Stoxx_50_Stock_Price_output.csv\",\n",
    "    \"Google_Stock_Price_output.csv\",\n",
    "    \"Hang_Seng_Stock_Price_output.csv\",\n",
    "    \"IBEX_35_Stock_Price_output.csv\",\n",
    "    \"Indra_Stock_Price_output.csv\",\n",
    "    \"P&G_Stock_Price_output.csv\",\n",
    "    \"S&P500_Stock_Price_output.csv\"\n",
    "]\n",
    "\n",
    "N_STEPS = 30\n",
    "FEATURE_COLUMNS = [\n",
    "    \"Open\",\"High\",\"Low\",\"Volume\",\n",
    "    \"EMA_7\",\"EMA_40\",\"MACD\",\"Signal_Line\",\n",
    "    \"MACD_Hist\",\"RSI\",\"ATR\"\n",
    "]\n",
    "TARGET_COLUMN = \"Close\"\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# FUNCIONES AUXILIARES\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def create_sequences(X, y, n_steps=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(n_steps, len(X)):\n",
    "        Xs.append(X[i - n_steps:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def build_lstm_model(sequence_length, n_features, units, n_layers, dropout, learning_rate):\n",
    "    \"\"\"\n",
    "    Reconstruye la arquitectura LSTM con los hiperparámetros dados.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        is_first = (i == 0)\n",
    "        is_last = (i == n_layers - 1)\n",
    "        return_sequences = not is_last\n",
    "        if is_first:\n",
    "            model.add(LSTM(\n",
    "                units, return_sequences=return_sequences,\n",
    "                input_shape=(sequence_length, n_features)\n",
    "            ))\n",
    "        else:\n",
    "            model.add(LSTM(units, return_sequences=return_sequences))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"huber\", optimizer=optimizer, metrics=[\"mean_absolute_error\"])\n",
    "    return model\n",
    "\n",
    "def compute_metrics(model, X_scaled, y_scaled, scaler_y):\n",
    "    \"\"\"\n",
    "    Calcula MAE, RMSE, R² y devuelve y_true, y_pred.\n",
    "    \"\"\"\n",
    "    y_pred_scaled = model.predict(X_scaled, verbose=0)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    y_true = scaler_y.inverse_transform(y_scaled)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2, y_true, y_pred\n",
    "\n",
    "def recompute_indicators(df):\n",
    "    \"\"\"\n",
    "    Recalcula EMA_7, EMA_40, MACD, Signal_Line, MACD_Hist, RSI, ATR para todo df.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    df['EMA_7'] = close.ewm(span=7, adjust=False).mean()\n",
    "    df['EMA_40'] = close.ewm(span=40, adjust=False).mean()\n",
    "\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    macd = ema_12 - ema_26\n",
    "    signal = macd.ewm(span=9, adjust=False).mean()\n",
    "    df['MACD'] = macd\n",
    "    df['Signal_Line'] = signal\n",
    "    df['MACD_Hist'] = macd - signal\n",
    "\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-8)\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df['ATR'] = (df['High'] - df['Low']).rolling(window=14).mean()\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# BUCLE PRINCIPAL: CARGAR CADA DATASET, LEER PESOS Y GRAFICAR\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "for filename in DATASETS:\n",
    "    dataset_name = os.path.splitext(filename)[0]\n",
    "    print(f\"\\n===== Procesando: {dataset_name} =====\")\n",
    "\n",
    "    # Rutas\n",
    "    data_path    = os.path.join(BASE_PATH, filename)\n",
    "    model_folder = os.path.join(RESULTS_PATH, dataset_name)\n",
    "    weights_path = os.path.join(model_folder, f\"{dataset_name}_best_weights.weights.h5\")\n",
    "    params_path  = os.path.join(model_folder, f\"{dataset_name}_best_params.json\")\n",
    "    \n",
    "    print((weights_path))\n",
    "    print(params_path)\n",
    "\n",
    "    if not os.path.isfile(weights_path) or not os.path.isfile(params_path):\n",
    "        print(f\"  ⚠️ Faltan pesos o parámetros para {dataset_name}, omitiendo.\")\n",
    "        continue\n",
    "\n",
    "    # 1) Cargar CSV original\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.sort_values(\"Date\", inplace=True)\n",
    "    df = df.dropna(subset=FEATURE_COLUMNS + [TARGET_COLUMN]).reset_index(drop=True)\n",
    "\n",
    "    # 2) Crear secuencias y dividir en test (solo test nos interesa)\n",
    "    data_X = df[FEATURE_COLUMNS].values\n",
    "    data_y = df[TARGET_COLUMN].values.reshape(-1, 1)\n",
    "    X_seq, y_seq = create_sequences(data_X, data_y, n_steps=N_STEPS)\n",
    "\n",
    "    n_total = len(X_seq)\n",
    "    test_start = int(n_total * (1 - TEST_RATIO))\n",
    "    X_test = X_seq[test_start:]\n",
    "    y_test = y_seq[test_start:]\n",
    "\n",
    "    # 3) Reconstruir scalers según train implícito (solo necesitamos scaler_X y scaler_y)\n",
    "    #    Para reconstruir exactamente el escalado, lo más sencillo es recalcular:\n",
    "    #    - Fit scaler_X sobre todo X_seq[:-len(X_test)] (train+val), \n",
    "    #    - Fit scaler_y sobre y_seq[:-len(y_test)].\n",
    "    split_point = test_start  # cantidad de secuencias de train+val\n",
    "    X_train_val = X_seq[:split_point]\n",
    "    y_train_val = y_seq[:split_point]\n",
    "\n",
    "    scaler_X = MinMaxScaler()\n",
    "    flat_Xtv = X_train_val.reshape(-1, len(FEATURE_COLUMNS))\n",
    "    scaler_X.fit(flat_Xtv)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    scaler_y.fit(y_train_val)\n",
    "\n",
    "    def scale_X(X):\n",
    "        flat = X.reshape(-1, len(FEATURE_COLUMNS))\n",
    "        flat_scaled = scaler_X.transform(flat)\n",
    "        return flat_scaled.reshape(X.shape)\n",
    "\n",
    "    X_test_scaled = scale_X(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    # 4) Cargar hiperparámetros\n",
    "    with open(params_path, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    n_layers      = best_params['n_layers']\n",
    "    units         = best_params['units']\n",
    "    dropout       = best_params['dropout']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    print(f\"  ✓ Parámetros cargados: layers={n_layers}, units={units}, dropout={dropout}, lr={learning_rate}\")\n",
    "\n",
    "    # 5) Reconstruir modelo y cargar pesos\n",
    "    model = build_lstm_model(\n",
    "        sequence_length=N_STEPS,\n",
    "        n_features=len(FEATURE_COLUMNS),\n",
    "        units=units,\n",
    "        n_layers=n_layers,\n",
    "        dropout=dropout,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    model.load_weights(weights_path)\n",
    "    print(f\"  ✓ Pesos cargados desde: {weights_path}\")\n",
    "\n",
    "    # 6) Calcular métricas en Test\n",
    "    mae_test, rmse_test, r2_test, y_true, y_pred = compute_metrics(\n",
    "        model, X_test_scaled, y_test_scaled, scaler_y\n",
    "    )\n",
    "    print(f\"  → Test MAE={mae_test:.4f}, RMSE={rmse_test:.4f}, R²={r2_test:.4f}\")\n",
    "\n",
    "    # 7) Graficar Real vs Predicho (Test)\n",
    "    dates_test = df['Date'].iloc[-len(y_true):].reset_index(drop=True)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dates_test, y=y_true.flatten(),\n",
    "        mode='lines', name='Real (Close)', line=dict(color='blue')\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dates_test, y=y_pred.flatten(),\n",
    "        mode='lines', name='Predicho', line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"{dataset_name} – Real vs Predicción (Test)\",\n",
    "        xaxis_title='Fecha',\n",
    "        yaxis_title='Precio Close (USD)',\n",
    "        template='plotly_dark',\n",
    "        xaxis_rangeslider_visible=True\n",
    "    )\n",
    "    plot_html = os.path.join(model_folder, f\"{dataset_name}_test_plot.html\")\n",
    "    plot_png  = os.path.join(model_folder, f\"{dataset_name}_test_plot.png\")\n",
    "    fig.write_html(plot_html)\n",
    "    fig.write_image(plot_png)\n",
    "    print(f\"  ✓ Gráfica Test guardada en: {plot_html}, {plot_png}\")\n",
    "\n",
    "    # 8) Predicción autoregresiva de los próximos 10 días laborables\n",
    "    df_future = df.copy().reset_index(drop=True)\n",
    "    recompute_indicators(df_future)\n",
    "    input_window = X_test_scaled[-1].reshape(1, N_STEPS, len(FEATURE_COLUMNS))\n",
    "    last_date = df_future['Date'].iloc[-1]\n",
    "    future_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=10)\n",
    "    future_preds = []\n",
    "\n",
    "    for date in future_dates:\n",
    "        y_pred_scaled = model.predict(input_window, verbose=0)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled)[0, 0]\n",
    "        future_preds.append(y_pred)\n",
    "\n",
    "        prev = df_future.iloc[-1]\n",
    "        new_row = {\n",
    "            'Date':        date,\n",
    "            'Open':        prev['Close'],\n",
    "            'High':        y_pred,\n",
    "            'Low':         y_pred,\n",
    "            'Volume':      prev['Volume'],\n",
    "            'Close':       y_pred,\n",
    "            'EMA_7':       np.nan,\n",
    "            'EMA_40':      np.nan,\n",
    "            'MACD':        np.nan,\n",
    "            'Signal_Line': np.nan,\n",
    "            'MACD_Hist':   np.nan,\n",
    "            'RSI':         np.nan,\n",
    "            'ATR':         np.nan\n",
    "        }\n",
    "        df_future.loc[len(df_future)] = new_row\n",
    "        recompute_indicators(df_future)\n",
    "\n",
    "        last_features = df_future[FEATURE_COLUMNS].iloc[-N_STEPS:].values\n",
    "        last_features_scaled = scaler_X.transform(last_features)\n",
    "        input_window = last_features_scaled.reshape(1, N_STEPS, len(FEATURE_COLUMNS))\n",
    "\n",
    "    # 9) Guardar predicciones futuras en CSV\n",
    "    df_fut_pred = pd.DataFrame({\n",
    "        \"Date\": future_dates,\n",
    "        \"Predicted_Close\": future_preds\n",
    "    })\n",
    "    fut_csv = os.path.join(model_folder, f\"{dataset_name}_future_10days.csv\")\n",
    "    df_fut_pred.to_csv(fut_csv, index=False)\n",
    "    print(f\"  ✓ Predicciones futuras guardadas en: {fut_csv}\")\n",
    "\n",
    "    # 10) Graficar histórico + predicciones futuras\n",
    "    fig_future = go.Figure()\n",
    "    fig_future.add_trace(go.Scatter(\n",
    "        x=df['Date'], y=df['Close'],\n",
    "        mode='lines', name='Histórico Close', line=dict(color='lightblue')\n",
    "    ))\n",
    "    fig_future.add_trace(go.Scatter(\n",
    "        x=future_dates, y=np.array(future_preds),\n",
    "        mode='lines+markers', name='Predicción futura',\n",
    "        line=dict(color='orange', dash='dash'),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "    fig_future.update_layout(\n",
    "        title=f\"{dataset_name} – Predicción Próximos 10 Días\",\n",
    "        xaxis_title='Fecha',\n",
    "        yaxis_title='Precio Close (USD)',\n",
    "        template='plotly_dark',\n",
    "        xaxis_rangeslider_visible=True\n",
    "    )\n",
    "    fut_html = os.path.join(model_folder, f\"{dataset_name}_future_plot.html\")\n",
    "    fut_png  = os.path.join(model_folder, f\"{dataset_name}_future_plot.png\")\n",
    "    fig_future.write_html(fut_html)\n",
    "    fig_future.write_image(fut_png)\n",
    "    print(f\"  ✓ Gráfica futura guardada en: {fut_html}, {fut_png}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
